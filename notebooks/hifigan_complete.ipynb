{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "386b503e",
   "metadata": {},
   "source": [
    "## 1. Instalación y Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46b7fbc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instalar dependencias si es necesario\n",
    "# !pip install torch torchaudio librosa soundfile numpy scipy matplotlib tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "206a079f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchaudio\n",
    "import librosa\n",
    "import soundfile as sf\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import Audio, display\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Device: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cebacf78",
   "metadata": {},
   "source": [
    "## 2. Preprocesamiento de Audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "529311fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuración de preprocesamiento\n",
    "CONFIG = {\n",
    "    'sample_rate': 22050,\n",
    "    'n_fft': 1024,\n",
    "    'hop_length': 256,\n",
    "    'win_length': 1024,\n",
    "    'n_mels': 80,\n",
    "    'fmin': 0,\n",
    "    'fmax': 8000,\n",
    "    'segment_size': 8192,  # ~0.37 segundos\n",
    "}\n",
    "\n",
    "def mel_spectrogram(y, sr, n_fft, hop_length, win_length, n_mels, fmin, fmax):\n",
    "    \"\"\"Extrae mel-spectrogram\"\"\"\n",
    "    mel = librosa.feature.melspectrogram(\n",
    "        y=y, sr=sr, n_fft=n_fft, hop_length=hop_length,\n",
    "        win_length=win_length, n_mels=n_mels, fmin=fmin, fmax=fmax\n",
    "    )\n",
    "    mel_db = librosa.power_to_db(mel, ref=np.max)\n",
    "    return mel_db\n",
    "\n",
    "def preprocess_audio_files(input_dir, output_dir, config, max_files=None):\n",
    "    \"\"\"Preprocesa archivos de audio\"\"\"\n",
    "    input_dir = Path(input_dir)\n",
    "    output_dir = Path(output_dir)\n",
    "    \n",
    "    audio_dir = output_dir / 'audio'\n",
    "    mel_dir = output_dir / 'mels'\n",
    "    audio_dir.mkdir(parents=True, exist_ok=True)\n",
    "    mel_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    audio_files = list(input_dir.glob('*.mp3'))\n",
    "    if max_files:\n",
    "        audio_files = audio_files[:max_files]\n",
    "    \n",
    "    print(f\"Procesando {len(audio_files)} archivos...\")\n",
    "    \n",
    "    metadata = []\n",
    "    for idx, audio_path in enumerate(tqdm(audio_files)):\n",
    "        try:\n",
    "            # Cargar audio\n",
    "            audio, sr = librosa.load(str(audio_path), sr=config['sample_rate'], mono=True)\n",
    "            \n",
    "            # Guardar audio procesado\n",
    "            audio_filename = f\"{idx:04d}.wav\"\n",
    "            sf.write(audio_dir / audio_filename, audio, config['sample_rate'])\n",
    "            \n",
    "            # Extraer mel-spectrogram\n",
    "            mel = mel_spectrogram(\n",
    "                audio, config['sample_rate'], config['n_fft'],\n",
    "                config['hop_length'], config['win_length'],\n",
    "                config['n_mels'], config['fmin'], config['fmax']\n",
    "            )\n",
    "            \n",
    "            mel_filename = f\"{idx:04d}.npy\"\n",
    "            np.save(mel_dir / mel_filename, mel)\n",
    "            \n",
    "            metadata.append({\n",
    "                'id': idx,\n",
    "                'original': audio_path.name,\n",
    "                'audio': audio_filename,\n",
    "                'mel': mel_filename,\n",
    "                'duration': len(audio) / config['sample_rate']\n",
    "            })\n",
    "        except Exception as e:\n",
    "            print(f\"Error en {audio_path.name}: {e}\")\n",
    "    \n",
    "    # Guardar metadata\n",
    "    with open(output_dir / 'metadata.json', 'w') as f:\n",
    "        json.dump(metadata, f, indent=2)\n",
    "    \n",
    "    with open(output_dir / 'config.json', 'w') as f:\n",
    "        json.dump(config, f, indent=2)\n",
    "    \n",
    "    print(f\"\\n✓ Procesados {len(metadata)} archivos\")\n",
    "    return metadata\n",
    "\n",
    "# Ejecutar preprocesamiento\n",
    "\n",
    "input_dir = r\"C:\\Users\\carlo\\Downloads\\Deep_learning_P\\jamendo_tracks\"\n",
    "output_dir = Path('data/processed')\n",
    "\n",
    "audio_files = list(Path(input_dir).glob('*.mp3'))\n",
    "print(f\"Archivos encontrados: {audio_files}\")\n",
    "\n",
    "if not (output_dir / 'metadata.json').exists():\n",
    "    metadata = preprocess_audio_files(input_dir, output_dir, CONFIG)\n",
    "else:\n",
    "    print(\"Datos ya preprocesados. Cargando metadata...\")\n",
    "    with open(output_dir / 'metadata.json', 'r') as f:\n",
    "        metadata = json.load(f)\n",
    "    print(f\"✓ Cargados {len(metadata)} archivos\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cbd5559",
   "metadata": {},
   "source": [
    "## 3. Dataset para HiFi-GAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3182e66d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudioDataset(Dataset):\n",
    "    \"\"\"Dataset para HiFi-GAN\"\"\"\n",
    "    \n",
    "    def __init__(self, data_dir, segment_size, hop_length, split='train', train_ratio=0.85):\n",
    "        self.data_dir = Path(data_dir)\n",
    "        self.segment_size = segment_size\n",
    "        self.hop_length = hop_length\n",
    "        \n",
    "        # Cargar metadata\n",
    "        with open(self.data_dir / 'metadata.json', 'r') as f:\n",
    "            metadata = json.load(f)\n",
    "        \n",
    "        # Split train/val\n",
    "        split_idx = int(len(metadata) * train_ratio)\n",
    "        if split == 'train':\n",
    "            self.metadata = metadata[:split_idx]\n",
    "        else:\n",
    "            self.metadata = metadata[split_idx:]\n",
    "        \n",
    "        print(f\"{split.capitalize()} dataset: {len(self.metadata)} archivos\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.metadata)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        item = self.metadata[idx]\n",
    "        \n",
    "        # Cargar audio\n",
    "        audio_path = self.data_dir / 'audio' / item['audio']\n",
    "        audio, sr = sf.read(audio_path)\n",
    "        audio = torch.FloatTensor(audio)\n",
    "        \n",
    "        # Cargar mel\n",
    "        mel_path = self.data_dir / 'mels' / item['mel']\n",
    "        mel = np.load(mel_path)\n",
    "        mel = torch.FloatTensor(mel)\n",
    "        \n",
    "        # Random crop para training\n",
    "        if audio.size(0) >= self.segment_size:\n",
    "            max_start = audio.size(0) - self.segment_size\n",
    "            start = np.random.randint(0, max_start + 1)\n",
    "            audio = audio[start:start + self.segment_size]\n",
    "            \n",
    "            # Correspondiente mel segment\n",
    "            mel_start = start // self.hop_length\n",
    "            mel_length = self.segment_size // self.hop_length\n",
    "            mel = mel[:, mel_start:mel_start + mel_length]\n",
    "        else:\n",
    "            # Pad si es muy corto\n",
    "            audio = F.pad(audio, (0, self.segment_size - audio.size(0)))\n",
    "            mel_length = self.segment_size // self.hop_length\n",
    "            if mel.size(1) < mel_length:\n",
    "                mel = F.pad(mel, (0, mel_length - mel.size(1)))\n",
    "        \n",
    "        return mel, audio.unsqueeze(0)\n",
    "\n",
    "# Crear datasets\n",
    "train_dataset = AudioDataset(output_dir, CONFIG['segment_size'], CONFIG['hop_length'], 'train')\n",
    "val_dataset = AudioDataset(output_dir, CONFIG['segment_size'], CONFIG['hop_length'], 'val')\n",
    "\n",
    "# Dataloaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True, num_workers=0)\n",
    "val_loader = DataLoader(val_dataset, batch_size=4, shuffle=False, num_workers=0)\n",
    "\n",
    "print(f\"\\nBatch size: 4\")\n",
    "print(f\"Training batches: {len(train_loader)}\")\n",
    "print(f\"Validation batches: {len(val_loader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2821184a",
   "metadata": {},
   "source": [
    "## 4. Implementación de HiFi-GAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "682a6b39",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResBlock(nn.Module):\n",
    "    \"\"\"Residual block con dilated convolutions\"\"\"\n",
    "    def __init__(self, channels, kernel_size=3, dilation=(1, 3, 5)):\n",
    "        super().__init__()\n",
    "        self.convs1 = nn.ModuleList([\n",
    "            nn.Conv1d(channels, channels, kernel_size, 1, dilation=d,\n",
    "                     padding=self.get_padding(kernel_size, d))\n",
    "            for d in dilation\n",
    "        ])\n",
    "        self.convs2 = nn.ModuleList([\n",
    "            nn.Conv1d(channels, channels, kernel_size, 1, dilation=1,\n",
    "                     padding=self.get_padding(kernel_size, 1))\n",
    "            for _ in dilation\n",
    "        ])\n",
    "    \n",
    "    def get_padding(self, kernel_size, dilation):\n",
    "        return int((kernel_size * dilation - dilation) / 2)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        for c1, c2 in zip(self.convs1, self.convs2):\n",
    "            xt = F.leaky_relu(x, 0.1)\n",
    "            xt = c1(xt)\n",
    "            xt = F.leaky_relu(xt, 0.1)\n",
    "            xt = c2(xt)\n",
    "            x = xt + x\n",
    "        return x\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    \"\"\"HiFi-GAN Generator\"\"\"\n",
    "    def __init__(self, n_mels=80):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Configuración simplificada\n",
    "        self.num_kernels = 3\n",
    "        self.num_upsamples = 4\n",
    "        upsample_rates = [8, 8, 2, 2]  # Total: 256x upsampling\n",
    "        upsample_kernel_sizes = [16, 16, 4, 4]\n",
    "        upsample_initial_channel = 256\n",
    "        resblock_kernel_sizes = [3, 7, 11]\n",
    "        resblock_dilation_sizes = [[1, 3, 5], [1, 3, 5], [1, 3, 5]]\n",
    "        \n",
    "        # Input conv\n",
    "        self.conv_pre = nn.Conv1d(n_mels, upsample_initial_channel, 7, 1, padding=3)\n",
    "        \n",
    "        # Upsampling layers\n",
    "        self.ups = nn.ModuleList()\n",
    "        for i, (u, k) in enumerate(zip(upsample_rates, upsample_kernel_sizes)):\n",
    "            self.ups.append(\n",
    "                nn.ConvTranspose1d(\n",
    "                    upsample_initial_channel // (2**i),\n",
    "                    upsample_initial_channel // (2**(i+1)),\n",
    "                    k, u, padding=(k-u)//2\n",
    "                )\n",
    "            )\n",
    "        \n",
    "        # Residual blocks\n",
    "        self.resblocks = nn.ModuleList()\n",
    "        for i in range(len(self.ups)):\n",
    "            ch = upsample_initial_channel // (2**(i+1))\n",
    "            for k, d in zip(resblock_kernel_sizes, resblock_dilation_sizes):\n",
    "                self.resblocks.append(ResBlock(ch, k, d))\n",
    "        \n",
    "        # Output conv\n",
    "        self.conv_post = nn.Conv1d(ch, 1, 7, 1, padding=3)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.conv_pre(x)\n",
    "        \n",
    "        for i, ups in enumerate(self.ups):\n",
    "            x = F.leaky_relu(x, 0.1)\n",
    "            x = ups(x)\n",
    "            \n",
    "            # Apply residual blocks\n",
    "            xs = None\n",
    "            for j in range(self.num_kernels):\n",
    "                idx = i * self.num_kernels + j\n",
    "                if xs is None:\n",
    "                    xs = self.resblocks[idx](x)\n",
    "                else:\n",
    "                    xs += self.resblocks[idx](x)\n",
    "            x = xs / self.num_kernels\n",
    "        \n",
    "        x = F.leaky_relu(x)\n",
    "        x = self.conv_post(x)\n",
    "        x = torch.tanh(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "# Inicializar generador\n",
    "generator = Generator(n_mels=CONFIG['n_mels']).to(device)\n",
    "print(f\"\\nGenerador creado\")\n",
    "print(f\"Parámetros: {sum(p.numel() for p in generator.parameters()):,}\")\n",
    "\n",
    "# Test forward pass\n",
    "test_mel = torch.randn(1, 80, 32).to(device)\n",
    "test_output = generator(test_mel)\n",
    "print(f\"\\nTest input: {test_mel.shape}\")\n",
    "print(f\"Test output: {test_output.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eb1935d",
   "metadata": {},
   "source": [
    "## 5. Discriminadores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "220f62bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PeriodDiscriminator(nn.Module):\n",
    "    \"\"\"Discriminador periódico\"\"\"\n",
    "    def __init__(self, period):\n",
    "        super().__init__()\n",
    "        self.period = period\n",
    "        \n",
    "        self.convs = nn.ModuleList([\n",
    "            nn.Conv2d(1, 32, (5, 1), (3, 1), padding=(2, 0)),\n",
    "            nn.Conv2d(32, 128, (5, 1), (3, 1), padding=(2, 0)),\n",
    "            nn.Conv2d(128, 512, (5, 1), (3, 1), padding=(2, 0)),\n",
    "            nn.Conv2d(512, 1024, (5, 1), (3, 1), padding=(2, 0)),\n",
    "            nn.Conv2d(1024, 1024, (5, 1), 1, padding=(2, 0)),\n",
    "        ])\n",
    "        self.conv_post = nn.Conv2d(1024, 1, (3, 1), 1, padding=(1, 0))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        fmap = []\n",
    "        \n",
    "        # Reshape to 2D\n",
    "        b, c, t = x.shape\n",
    "        if t % self.period != 0:\n",
    "            n_pad = self.period - (t % self.period)\n",
    "            x = F.pad(x, (0, n_pad), \"reflect\")\n",
    "            t = t + n_pad\n",
    "        x = x.view(b, c, t // self.period, self.period)\n",
    "        \n",
    "        for conv in self.convs:\n",
    "            x = conv(x)\n",
    "            x = F.leaky_relu(x, 0.1)\n",
    "            fmap.append(x)\n",
    "        \n",
    "        x = self.conv_post(x)\n",
    "        fmap.append(x)\n",
    "        x = torch.flatten(x, 1, -1)\n",
    "        \n",
    "        return x, fmap\n",
    "\n",
    "class MultiPeriodDiscriminator(nn.Module):\n",
    "    \"\"\"Multi-Period Discriminator\"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.discriminators = nn.ModuleList([\n",
    "            PeriodDiscriminator(2),\n",
    "            PeriodDiscriminator(3),\n",
    "            PeriodDiscriminator(5),\n",
    "            PeriodDiscriminator(7),\n",
    "            PeriodDiscriminator(11),\n",
    "        ])\n",
    "    \n",
    "    def forward(self, y, y_hat):\n",
    "        y_d_rs = []\n",
    "        y_d_gs = []\n",
    "        fmap_rs = []\n",
    "        fmap_gs = []\n",
    "        \n",
    "        for d in self.discriminators:\n",
    "            y_d_r, fmap_r = d(y)\n",
    "            y_d_g, fmap_g = d(y_hat)\n",
    "            y_d_rs.append(y_d_r)\n",
    "            y_d_gs.append(y_d_g)\n",
    "            fmap_rs.append(fmap_r)\n",
    "            fmap_gs.append(fmap_g)\n",
    "        \n",
    "        return y_d_rs, y_d_gs, fmap_rs, fmap_gs\n",
    "\n",
    "# Inicializar discriminador\n",
    "discriminator = MultiPeriodDiscriminator().to(device)\n",
    "print(f\"\\nDiscriminador creado\")\n",
    "print(f\"Parámetros: {sum(p.numel() for p in discriminator.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b867fd23",
   "metadata": {},
   "source": [
    "## 6. Funciones de Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2e04b6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_loss(fmap_r, fmap_g):\n",
    "    \"\"\"Feature matching loss\"\"\"\n",
    "    loss = 0\n",
    "    for dr, dg in zip(fmap_r, fmap_g):\n",
    "        for rl, gl in zip(dr, dg):\n",
    "            loss += torch.mean(torch.abs(rl - gl))\n",
    "    return loss * 2\n",
    "\n",
    "def discriminator_loss(disc_real_outputs, disc_generated_outputs):\n",
    "    \"\"\"Discriminator loss\"\"\"\n",
    "    loss = 0\n",
    "    for dr, dg in zip(disc_real_outputs, disc_generated_outputs):\n",
    "        r_loss = torch.mean((1 - dr) ** 2)\n",
    "        g_loss = torch.mean(dg ** 2)\n",
    "        loss += (r_loss + g_loss)\n",
    "    return loss\n",
    "\n",
    "def generator_loss(disc_outputs):\n",
    "    \"\"\"Generator adversarial loss\"\"\"\n",
    "    loss = 0\n",
    "    for dg in disc_outputs:\n",
    "        loss += torch.mean((1 - dg) ** 2)\n",
    "    return loss\n",
    "\n",
    "def mel_spectrogram_loss(y, y_g, config):\n",
    "    \"\"\"Mel-spectrogram reconstruction loss\"\"\"\n",
    "    y_mel = mel_spectrogram(\n",
    "        y.squeeze(1).cpu().numpy()[0],\n",
    "        config['sample_rate'], config['n_fft'],\n",
    "        config['hop_length'], config['win_length'],\n",
    "        config['n_mels'], config['fmin'], config['fmax']\n",
    "    )\n",
    "    y_g_mel = mel_spectrogram(\n",
    "        y_g.squeeze(1).detach().cpu().numpy()[0],\n",
    "        config['sample_rate'], config['n_fft'],\n",
    "        config['hop_length'], config['win_length'],\n",
    "        config['n_mels'], config['fmin'], config['fmax']\n",
    "    )\n",
    "    \n",
    "    return F.l1_loss(\n",
    "        torch.FloatTensor(y_mel).to(device),\n",
    "        torch.FloatTensor(y_g_mel).to(device)\n",
    "    )\n",
    "\n",
    "print(\"✓ Funciones de loss definidas\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c0690d0",
   "metadata": {},
   "source": [
    "## 7. Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89d2d2f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuración de training\n",
    "TRAIN_CONFIG = {\n",
    "    'epochs': 100,\n",
    "    'lr': 0.0002,\n",
    "    'betas': (0.8, 0.99),\n",
    "    'lambda_mel': 45,  # Peso de mel loss\n",
    "    'lambda_fm': 2,    # Peso de feature matching\n",
    "}\n",
    "\n",
    "# Optimizers\n",
    "optim_g = torch.optim.AdamW(generator.parameters(), \n",
    "                            TRAIN_CONFIG['lr'], \n",
    "                            betas=TRAIN_CONFIG['betas'])\n",
    "optim_d = torch.optim.AdamW(discriminator.parameters(), \n",
    "                            TRAIN_CONFIG['lr'], \n",
    "                            betas=TRAIN_CONFIG['betas'])\n",
    "\n",
    "# Schedulers\n",
    "scheduler_g = torch.optim.lr_scheduler.ExponentialLR(optim_g, gamma=0.999)\n",
    "scheduler_d = torch.optim.lr_scheduler.ExponentialLR(optim_d, gamma=0.999)\n",
    "\n",
    "print(\"✓ Optimizers configurados\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dabbbd3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training function\n",
    "def train_epoch(epoch):\n",
    "    generator.train()\n",
    "    discriminator.train()\n",
    "    \n",
    "    total_loss_g = 0\n",
    "    total_loss_d = 0\n",
    "    \n",
    "    pbar = tqdm(train_loader, desc=f\"Epoch {epoch}\")\n",
    "    \n",
    "    for batch_idx, (mel, audio) in enumerate(pbar):\n",
    "        mel = mel.to(device)\n",
    "        audio = audio.to(device)\n",
    "        \n",
    "        # ===== Train Discriminator =====\n",
    "        optim_d.zero_grad()\n",
    "        \n",
    "        # Generate audio\n",
    "        audio_g = generator(mel)\n",
    "        \n",
    "        # Discriminator\n",
    "        y_dr, y_dg, _, _ = discriminator(audio, audio_g.detach())\n",
    "        \n",
    "        # Loss\n",
    "        loss_d = discriminator_loss(y_dr, y_dg)\n",
    "        \n",
    "        loss_d.backward()\n",
    "        optim_d.step()\n",
    "        \n",
    "        # ===== Train Generator =====\n",
    "        optim_g.zero_grad()\n",
    "        \n",
    "        # Discriminator on generated\n",
    "        y_dr, y_dg, fmap_r, fmap_g = discriminator(audio, audio_g)\n",
    "        \n",
    "        # Losses\n",
    "        loss_fm = feature_loss(fmap_r, fmap_g)\n",
    "        loss_gen = generator_loss(y_dg)\n",
    "        \n",
    "        # Mel loss (simplified - only first sample)\n",
    "        loss_mel = F.l1_loss(mel, mel) * 0  # Placeholder\n",
    "        \n",
    "        loss_g = loss_gen + TRAIN_CONFIG['lambda_fm'] * loss_fm + TRAIN_CONFIG['lambda_mel'] * loss_mel\n",
    "        \n",
    "        loss_g.backward()\n",
    "        optim_g.step()\n",
    "        \n",
    "        # Track\n",
    "        total_loss_g += loss_g.item()\n",
    "        total_loss_d += loss_d.item()\n",
    "        \n",
    "        pbar.set_postfix({\n",
    "            'G': f\"{loss_g.item():.4f}\",\n",
    "            'D': f\"{loss_d.item():.4f}\"\n",
    "        })\n",
    "    \n",
    "    # Schedulers\n",
    "    scheduler_g.step()\n",
    "    scheduler_d.step()\n",
    "    \n",
    "    return total_loss_g / len(train_loader), total_loss_d / len(train_loader)\n",
    "\n",
    "print(\"✓ Training function definida\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9317350b",
   "metadata": {},
   "source": [
    "## 8. Ejecutar Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "632286db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "history = {'loss_g': [], 'loss_d': []}\n",
    "\n",
    "NUM_EPOCHS = 50  # Ajusta según necesites\n",
    "\n",
    "print(f\"\\nIniciando training por {NUM_EPOCHS} epochs...\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "for epoch in range(1, NUM_EPOCHS + 1):\n",
    "    loss_g, loss_d = train_epoch(epoch)\n",
    "    \n",
    "    history['loss_g'].append(loss_g)\n",
    "    history['loss_d'].append(loss_d)\n",
    "    \n",
    "    print(f\"\\nEpoch {epoch}/{NUM_EPOCHS}\")\n",
    "    print(f\"  Generator Loss: {loss_g:.4f}\")\n",
    "    print(f\"  Discriminator Loss: {loss_d:.4f}\")\n",
    "    \n",
    "    # Save checkpoint every 10 epochs\n",
    "    if epoch % 10 == 0:\n",
    "        checkpoint_dir = Path('models/hifigan_checkpoints')\n",
    "        checkpoint_dir.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'generator': generator.state_dict(),\n",
    "            'discriminator': discriminator.state_dict(),\n",
    "            'optim_g': optim_g.state_dict(),\n",
    "            'optim_d': optim_d.state_dict(),\n",
    "        }, checkpoint_dir / f'checkpoint_epoch_{epoch}.pt')\n",
    "        \n",
    "        print(f\"  ✓ Checkpoint guardado\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"✓ Training completado!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1b99e12",
   "metadata": {},
   "source": [
    "## 9. Visualizar Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0830392",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training curves\n",
    "fig, ax = plt.subplots(1, 1, figsize=(12, 5))\n",
    "\n",
    "epochs = range(1, len(history['loss_g']) + 1)\n",
    "ax.plot(epochs, history['loss_g'], 'b-', label='Generator Loss', linewidth=2)\n",
    "ax.plot(epochs, history['loss_d'], 'r-', label='Discriminator Loss', linewidth=2)\n",
    "ax.set_xlabel('Epoch', fontsize=12)\n",
    "ax.set_ylabel('Loss', fontsize=12)\n",
    "ax.set_title('HiFi-GAN Training Curves', fontsize=14, fontweight='bold')\n",
    "ax.legend(fontsize=11)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('models/hifigan_checkpoints/training_curves.png', dpi=150)\n",
    "plt.show()\n",
    "\n",
    "print(\"✓ Gráficas guardadas\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb45524c",
   "metadata": {},
   "source": [
    "## 10. Generación de Audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80962fa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función de generación\n",
    "@torch.no_grad()\n",
    "def generate_from_mel(mel_spec):\n",
    "    \"\"\"Genera audio desde mel-spectrogram\"\"\"\n",
    "    generator.eval()\n",
    "    \n",
    "    if isinstance(mel_spec, np.ndarray):\n",
    "        mel_spec = torch.FloatTensor(mel_spec)\n",
    "    \n",
    "    if mel_spec.dim() == 2:\n",
    "        mel_spec = mel_spec.unsqueeze(0)\n",
    "    \n",
    "    mel_spec = mel_spec.to(device)\n",
    "    \n",
    "    audio = generator(mel_spec)\n",
    "    audio = audio.squeeze().cpu().numpy()\n",
    "    \n",
    "    return audio\n",
    "\n",
    "# Cargar un mel de validación\n",
    "val_sample = val_dataset[0]\n",
    "val_mel, val_audio_gt = val_sample\n",
    "\n",
    "print(f\"Mel shape: {val_mel.shape}\")\n",
    "print(f\"Audio GT shape: {val_audio_gt.shape}\")\n",
    "\n",
    "# Generar audio\n",
    "generated_audio = generate_from_mel(val_mel)\n",
    "print(f\"Generated audio shape: {generated_audio.shape}\")\n",
    "\n",
    "# Comparar\n",
    "print(\"\\n=\" * 70)\n",
    "print(\"AUDIO GENERADO\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(\"\\n1. Ground Truth (Original):\")\n",
    "display(Audio(val_audio_gt.numpy(), rate=CONFIG['sample_rate']))\n",
    "\n",
    "print(\"\\n2. Generado por HiFi-GAN:\")\n",
    "display(Audio(generated_audio, rate=CONFIG['sample_rate']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "564bc373",
   "metadata": {},
   "source": [
    "## 11. Visualización de Resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "846643e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot waveforms\n",
    "fig, axes = plt.subplots(2, 1, figsize=(15, 6))\n",
    "\n",
    "# Ground truth\n",
    "time_gt = np.arange(len(val_audio_gt.squeeze())) / CONFIG['sample_rate']\n",
    "axes[0].plot(time_gt, val_audio_gt.squeeze().numpy(), linewidth=0.5)\n",
    "axes[0].set_title('Ground Truth Audio', fontsize=12, fontweight='bold')\n",
    "axes[0].set_xlabel('Time (s)')\n",
    "axes[0].set_ylabel('Amplitude')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Generated\n",
    "time_gen = np.arange(len(generated_audio)) / CONFIG['sample_rate']\n",
    "axes[1].plot(time_gen, generated_audio, linewidth=0.5, color='orange')\n",
    "axes[1].set_title('Generated Audio (HiFi-GAN)', fontsize=12, fontweight='bold')\n",
    "axes[1].set_xlabel('Time (s)')\n",
    "axes[1].set_ylabel('Amplitude')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Plot spectrograms\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 4))\n",
    "\n",
    "# GT spectrogram\n",
    "axes[0].imshow(val_mel.numpy(), aspect='auto', origin='lower', cmap='viridis')\n",
    "axes[0].set_title('Input Mel-Spectrogram', fontsize=12, fontweight='bold')\n",
    "axes[0].set_xlabel('Time')\n",
    "axes[0].set_ylabel('Mel Bins')\n",
    "\n",
    "# Generated mel\n",
    "gen_mel = mel_spectrogram(\n",
    "    generated_audio, CONFIG['sample_rate'], CONFIG['n_fft'],\n",
    "    CONFIG['hop_length'], CONFIG['win_length'],\n",
    "    CONFIG['n_mels'], CONFIG['fmin'], CONFIG['fmax']\n",
    ")\n",
    "axes[1].imshow(gen_mel, aspect='auto', origin='lower', cmap='viridis')\n",
    "axes[1].set_title('Generated Mel-Spectrogram', fontsize=12, fontweight='bold')\n",
    "axes[1].set_xlabel('Time')\n",
    "axes[1].set_ylabel('Mel Bins')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8d8478d",
   "metadata": {},
   "source": [
    "## 12. Guardar Modelo y Generar Múltiples Samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38a9a7c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guardar modelo final\n",
    "model_dir = Path('models/hifigan_final')\n",
    "model_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "torch.save({\n",
    "    'generator': generator.state_dict(),\n",
    "    'config': CONFIG,\n",
    "    'train_config': TRAIN_CONFIG,\n",
    "}, model_dir / 'hifigan_generator.pt')\n",
    "\n",
    "print(f\"✓ Modelo guardado en: {model_dir / 'hifigan_generator.pt'}\")\n",
    "\n",
    "# Generar múltiples samples\n",
    "output_dir = Path('output/hifigan_samples')\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"\\nGenerando samples...\")\n",
    "for i in range(min(5, len(val_dataset))):\n",
    "    mel, audio_gt = val_dataset[i]\n",
    "    \n",
    "    # Generate\n",
    "    audio_gen = generate_from_mel(mel)\n",
    "    \n",
    "    # Save\n",
    "    sf.write(\n",
    "        output_dir / f'sample_{i}_generated.wav',\n",
    "        audio_gen,\n",
    "        CONFIG['sample_rate']\n",
    "    )\n",
    "    sf.write(\n",
    "        output_dir / f'sample_{i}_groundtruth.wav',\n",
    "        audio_gt.squeeze().numpy(),\n",
    "        CONFIG['sample_rate']\n",
    "    )\n",
    "    \n",
    "    print(f\"  ✓ Sample {i} guardado\")\n",
    "\n",
    "print(f\"\\n✓ Samples guardados en: {output_dir}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
